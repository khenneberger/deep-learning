{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "950b9b6f-3042-4fdc-96e3-e7854f580f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # math calcuations and other matrix, vector processing\n",
    "import pandas as pd # dataframe organization (similar to Excel)\n",
    "import seaborn as sns # for plotting\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from sklearn.preprocessing import StandardScaler #replace by MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Dropout, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9b8f9bb-2def-492b-a5ca-cbdbff6cdc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = 'train.csv'\n",
    "test_csv = 'test.csv'\n",
    "label_name = 'label'\n",
    "id_name = 'id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "105f7367-2e1a-4bb6-ae52-cdd13a60e077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def read_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    return df\n",
    "# collect distinct labels\n",
    "def distinct(df,label_name):\n",
    "    train_labels = df[label_name]\n",
    "    data_labels = train_labels.unique().tolist()\n",
    "    return data_labels, train_labels\n",
    "# convert train images to matrix\n",
    "def convert_train(df_train_labels, id_name, img_size, channels, X_train, y_train):\n",
    "    train_ids = df_train_labels[id_name]\n",
    "    for index, _train_id in enumerate(train_ids):\n",
    "        _label = train_labels[index]\n",
    "        y_train.append(data_labels.index(_label))\n",
    "        # image to matrix and resize to img_size * img_size    \n",
    "        _img = load_img(f'imgs/{_train_id}.jpeg', target_size=(img_size, img_size))\n",
    "        img2arr = img_to_array(_img)\n",
    "        X_train.append(img2arr)\n",
    "    # convert list to array\n",
    "    X_train = np.array(X_train, dtype=np.float32)/255\n",
    "    y_train = np.array(y_train)\n",
    "    return X_train, y_train\n",
    "# convert test images to matrix\n",
    "def convert_test(df_test, id_name, img_size, X_test):\n",
    "    test_ids = df_test[id_name]\n",
    "    for index, _test_id in enumerate(test_ids):\n",
    "        _label = train_labels[index]\n",
    "        _img = load_img(f'imgs/{_test_id}.jpeg', target_size=(img_size, img_size))\n",
    "        img2arr = img_to_array(_img)\n",
    "        X_test.append(img2arr)\n",
    "        # convert list to array and normalize\n",
    "        X_test = np.array(X_test, dtype=np.float32)/255\n",
    "        return X_test\n",
    "# Build CNN model\n",
    "def build_CNN_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3,3),\n",
    "                 strides=(1,1),\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=(48,48,3))\n",
    "             ) # output size: 48x48x64\n",
    "\n",
    "    model.add(BatchNormalization()) #optional\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size=(3,3),\n",
    "                 strides=(1,1),\n",
    "                 padding='valid',\n",
    "                 activation='relu'\n",
    "                 #nput_shape=(48,48,64)\n",
    "                    )\n",
    "             )\n",
    "    model.add(BatchNormalization()) #optional\n",
    "    model.add(MaxPooling2D(2,2)) #optional, but often used to reduce \n",
    "                             # the overfitting, get simpler representation\n",
    "    model.add(Dropout(0.25)) # optional, reduce overfitting \n",
    "\n",
    "    model.add(Conv2D(128, kernel_size=(3,3),\n",
    "                 strides=(1,1),\n",
    "                 padding='valid',\n",
    "                 activation='relu'                \n",
    "                    )\n",
    "             )       \n",
    "\n",
    "    model.add(BatchNormalization()) #optional\n",
    "    model.add(MaxPooling2D(2,2)) \n",
    "\n",
    "    model.add(Flatten())\n",
    "    #Fully Connected Layer(=Dense), 512, ReLU, BatchNorm, dropout(0.5)\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(BatchNormalization()) #optional\n",
    "    model.add(Dropout(0.5))\n",
    "    #-> Dense, 128, ReLU, BatchNorm, dropout(0.5)\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(BatchNormalization()) #optional\n",
    "    model.add(Dropout(0.5))\n",
    "    #Dense, 2, softmax (Output layer)\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d4a23c4-8fb8-49df-a0ff-ddeba3936eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Training labels and collect distinct labels\n",
    "df_train_labels = read_data(train_csv)\n",
    "data_labels, train_labels = distinct(df_train_labels, label_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a927cb76-9c62-4f79-81d6-e95e186bade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [] # matrix of train images\n",
    "y_train = [] # labels\n",
    "img_size = 48 # user-defined, original: 32x32\n",
    "channels = 3\n",
    "X_train, y_train = convert_train(df_train_labels, id_name, img_size, channels, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e6f5a7d-5072-4aa2-8637-8862c567c7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct validation data \n",
    "X_train_, X_val, y_train_, y_val = train_test_split(\n",
    "    X_train, y_train, stratify = y_train, test_size=0.3, \n",
    "    random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fdc7a0d9-b7ce-4795-97f3-ddd0fee75ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 48, 48, 64)        1792      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 48, 48, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 46, 46, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 46, 46, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 23, 23, 64)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 21, 21, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 21, 21, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               6554112   \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 128)               65664     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,736,194\n",
      "Trainable params: 6,734,402\n",
      "Non-trainable params: 1,792\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = build_CNN_model(X_train_[0].shape)\n",
    "model.summary()\n",
    "#compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer='Adam', metrics=['accuracy'])\n",
    "# optimizer: very important to get a better performance\n",
    "# Adagrad, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "828cf3ed-9745-4f66-8f44-703bcbef4a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "148/148 [==============================] - 83s 526ms/step - loss: 0.3691 - accuracy: 0.8841 - val_loss: 2.2440 - val_accuracy: 0.7336\n",
      "Epoch 2/50\n",
      "148/148 [==============================] - 78s 526ms/step - loss: 0.1920 - accuracy: 0.9365 - val_loss: 1.5387 - val_accuracy: 0.7336\n",
      "Epoch 3/50\n",
      "148/148 [==============================] - 77s 519ms/step - loss: 0.1989 - accuracy: 0.9315 - val_loss: 2.7161 - val_accuracy: 0.7336\n",
      "Epoch 4/50\n",
      "148/148 [==============================] - 77s 518ms/step - loss: 0.1566 - accuracy: 0.9473 - val_loss: 0.1816 - val_accuracy: 0.9281\n",
      "Epoch 5/50\n",
      "148/148 [==============================] - 77s 523ms/step - loss: 0.1287 - accuracy: 0.9558 - val_loss: 0.1399 - val_accuracy: 0.9422\n",
      "Epoch 6/50\n",
      "148/148 [==============================] - 78s 525ms/step - loss: 0.1089 - accuracy: 0.9602 - val_loss: 0.6394 - val_accuracy: 0.7822\n",
      "Epoch 7/50\n",
      "148/148 [==============================] - 77s 522ms/step - loss: 0.1048 - accuracy: 0.9626 - val_loss: 0.1426 - val_accuracy: 0.9507\n",
      "Epoch 8/50\n",
      "148/148 [==============================] - 76s 517ms/step - loss: 0.1126 - accuracy: 0.9600 - val_loss: 0.0641 - val_accuracy: 0.9789\n",
      "Epoch 9/50\n",
      "148/148 [==============================] - 77s 524ms/step - loss: 0.0849 - accuracy: 0.9698 - val_loss: 0.1147 - val_accuracy: 0.9563\n",
      "Epoch 10/50\n",
      "148/148 [==============================] - 77s 524ms/step - loss: 0.0788 - accuracy: 0.9729 - val_loss: 0.6430 - val_accuracy: 0.7886\n",
      "Epoch 11/50\n",
      "148/148 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.9695Restoring model weights from the end of the best epoch: 8.\n",
      "148/148 [==============================] - 77s 523ms/step - loss: 0.0911 - accuracy: 0.9695 - val_loss: 0.2242 - val_accuracy: 0.9281\n",
      "Epoch 11: early stopping\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=3, \n",
    "    verbose=1,\n",
    "    restore_best_weights=True                               \n",
    "    )\n",
    "data = {'X_train': X_train, 'y_train': y_train,\n",
    "        'X_val' : X_val, 'y_val': y_val}\n",
    "history = model.fit(x=X_train, y=y_train, \n",
    "                    batch_size=32,\n",
    "                    validation_data=[data['X_val'], data['y_val']],\n",
    "                    epochs=50,\n",
    "                    callbacks=[callback],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a23f8a99-e51b-4243-b058-85a0ef7b5376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 6s 129ms/step\n",
      "Accuracy score = 0.9788583509513742\n",
      "sum of val prediction = 390\n",
      "sum of val actual = 378\n"
     ]
    }
   ],
   "source": [
    "# check accuracy on the Test prediction\n",
    "# Check performance on test-train-split set:\n",
    "#predicted score\n",
    "y_pred_test = model.predict(X_val) #output of last layer\n",
    "# predicted labels\n",
    "y_pred_label_test = np.argmax(y_pred_test, axis=1)\n",
    "\n",
    "\n",
    "print(f'Accuracy score = {accuracy_score(y_val,y_pred_label_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9068cbc1-c961-408d-9d74-387fda138d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the labels of the test data \n",
    "df_test = read_data(test_csv)\n",
    "X_test = []\n",
    "X_test = convert_test(df_test, id_name, img_size, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5061201b-2563-4e35-8f5e-8113a06e5402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_prob_pred = model.predict(X_test)\n",
    "y_test_label_pred = np.argmax(y_test_prob_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4277840e-613a-4798-9c3d-1c357cead68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Predictions for Test Data\n",
    "#predicted score\n",
    "df_test_pred = pd.DataFrame(data=y_test_label_pred, columns=['label'])\n",
    "df_test_pred.to_csv('prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
