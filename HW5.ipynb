{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa600c20-2a43-4d22-ab61-d483aabae75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # math calcuations and other matrix, vector processing\n",
    "import pandas as pd # dataframe organization (similar to Excel)\n",
    "import seaborn as sns # for plotting\n",
    "import matplotlib.pyplot as plt # for plotting\n",
    "from sklearn.preprocessing import StandardScaler #replace by MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Embedding # embedding=dense=fully connected layer\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import RNN, GRU, LSTM\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad452d66-7854-413f-ad20-4338c949f969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('twitter_train.csv')\n",
    "df_test = pd.read_csv('twitter_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfa32e1e-88ce-457b-b66b-5e69be10f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lower case and remove non-word characters\n",
    "def preprocess_sentence(s):\n",
    "    # convert to lower\n",
    "    s = s.lower()\n",
    "    # remove all non-word characters (everything except numbers and letters)\n",
    "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
    "    return s\n",
    "# Determine size of vocabulary\n",
    "def get_vocabulary_size(tweets):\n",
    "    tweets = [preprocess_sentence(s) for s in tweets]\n",
    "    combined_tweets = ' '.join(tweets)\n",
    "    words = combined_tweets.split()\n",
    "    count_words = Counter(words)\n",
    "    total_words = len(words)\n",
    "    sorted_words = count_words.most_common(total_words)\n",
    "    return len(sorted_words), sorted_words\n",
    "# Tokenize the sentence\n",
    "def tokenize(tweets):\n",
    "    int_tweets = []\n",
    "    for twitter in tweets:\n",
    "        twitter = preprocess_sentence(twitter)\n",
    "        r = [word2int[w] for w in twitter.split()]\n",
    "        int_tweets.append(r)\n",
    "    return int_tweets\n",
    "# Pad by adding 0 at the end or truncate \n",
    "def padding(int_tweets, maximum_length=200):\n",
    "    padded_tweets = np.zeros((len(int_tweets), maximum_length), dtype=int)\n",
    "    for i, int_tweet in enumerate(int_tweets):\n",
    "        tweet_len = len(int_tweet)\n",
    "        if tweet_len >= maximum_length:\n",
    "            padded_tweets[i, :] = np.array(int_tweet[:maximum_length])\n",
    "        else:\n",
    "            _temp = int_tweet + list(np.zeros(maximum_length-tweet_len))\n",
    "            padded_tweets[i, :] = np.array(_temp)\n",
    "    return padded_tweets\n",
    "def get_model(embed_dim, hidden_state_dim):\n",
    "    model = Sequential()    \n",
    "    model.add(Embedding(input_dim=vocab_dim, #size of vocabulary\n",
    "                        output_dim=embed_dim, # size of output dimension\n",
    "                        input_length=X_train_pad.shape[-1] # max length of a sentence\n",
    "\n",
    "                    ))\n",
    "    # we can use drop out etc.\n",
    "    #model.add(GRU(hidden_state_dim, return_sequences=False))\n",
    "    #model.add(LSTM(hidden_state_dim, return_sequences=True))\n",
    "    #model.add(GRU(hidden_state_dim, return_sequences=False))\n",
    "    model.add(GRU(hidden_state_dim, return_sequences=True))\n",
    "    model.add(GRU(hidden_state_dim, return_sequences=True))\n",
    "    model.add(GRU(hidden_state_dim, return_sequences=True))\n",
    "    model.add(GRU(hidden_state_dim, return_sequences=False))\n",
    "    model.add(Dense(3, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76d37ac-396c-4b89-b16d-c07675e6bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = [preprocess_sentence(s) for s in df_train['twitter']]\n",
    "test_tweets = [preprocess_sentence(s) for s in df_test['twitter']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fa04ee-b13e-420e-b847-1f15dd16a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dim, sorted_words = get_vocabulary_size(train_tweets + \n",
    "                                             test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6038a722-4218-4267-9233-dff8e430d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2int = {w: i+1 for i, (w, c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d93fe9e4-e50e-44d8-8e52-6bdf6ae577be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenize(train_tweets)\n",
    "X_test = tokenize(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "717a31cb-d389-43fb-9fc3-1bcf8c8a2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert labels to integers\n",
    "le=LabelEncoder()\n",
    "y_train = le.fit_transform(df_train['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcae9d97-72f7-4ef2-8424-00d8c934b934",
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_length=50 # tweets are shorter than reviews we looked at in class\n",
    "X_train_pad = padding(X_train, maximum_length)\n",
    "X_test_pad = padding(X_test, maximum_length)\n",
    "vocab_dim = vocab_dim + 1 # +1 for padding 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a43c9f5-b9f4-40a6-a930-502071327eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 50, 20)            1583980   \n",
      "                                                                 \n",
      " gru_4 (GRU)                 (None, 50, 30)            4680      \n",
      "                                                                 \n",
      " gru_5 (GRU)                 (None, 50, 30)            5580      \n",
      "                                                                 \n",
      " gru_6 (GRU)                 (None, 50, 30)            5580      \n",
      "                                                                 \n",
      " gru_7 (GRU)                 (None, 30)                5580      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 93        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,605,493\n",
      "Trainable params: 1,605,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "2375/2375 [==============================] - 285s 113ms/step - loss: 0.4821 - accuracy: 0.8085 - val_loss: 0.2849 - val_accuracy: 0.9065\n",
      "Epoch 2/10\n",
      "2375/2375 [==============================] - 266s 112ms/step - loss: 0.2061 - accuracy: 0.9301 - val_loss: 0.2077 - val_accuracy: 0.9345\n",
      "Epoch 3/10\n",
      "2375/2375 [==============================] - 268s 113ms/step - loss: 0.1305 - accuracy: 0.9589 - val_loss: 0.1872 - val_accuracy: 0.9445\n",
      "Epoch 4/10\n",
      "2375/2375 [==============================] - 268s 113ms/step - loss: 0.0928 - accuracy: 0.9723 - val_loss: 0.1777 - val_accuracy: 0.9473\n",
      "Epoch 5/10\n",
      "2375/2375 [==============================] - 268s 113ms/step - loss: 0.0632 - accuracy: 0.9801 - val_loss: 0.1758 - val_accuracy: 0.9465\n",
      "Epoch 6/10\n",
      "2375/2375 [==============================] - 278s 117ms/step - loss: 0.0420 - accuracy: 0.9872 - val_loss: 0.1964 - val_accuracy: 0.9473\n",
      "Epoch 7/10\n",
      "2375/2375 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9907Restoring model weights from the end of the best epoch: 5.\n",
      "2375/2375 [==============================] - 273s 115ms/step - loss: 0.0304 - accuracy: 0.9907 - val_loss: 0.2276 - val_accuracy: 0.9477\n",
      "Epoch 7: early stopping\n"
     ]
    }
   ],
   "source": [
    "seed = 2022\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "embed_dim = 20\n",
    "hidden_state_dim = 30\n",
    "\n",
    "model = get_model(embed_dim, hidden_state_dim)\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy']\n",
    "             )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", \n",
    "    patience=2, \n",
    "    verbose=1,\n",
    "    restore_best_weights=True                               \n",
    "    )\n",
    "\n",
    "X_train_new , X_val_new, y_train_new, y_val_new = \\\n",
    "    train_test_split(X_train_pad, y_train, stratify= y_train,\n",
    "                                    test_size=0.05,random_state=2022)\n",
    "\n",
    "history = model.fit(x=X_train_new, y=y_train_new, \n",
    "                    batch_size=32,\n",
    "                    validation_data=[X_val_new, y_val_new],\n",
    "                    epochs=10,\n",
    "                    callbacks=[callback],\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d27548-770c-4453-9bbb-e825640bbde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 30s 48ms/step\n"
     ]
    }
   ],
   "source": [
    "# Save Test predictions \n",
    "y_test_pred = model.predict(X_test_pad)\n",
    "# change back to [-1, 0 , 1] values for sentiment\n",
    "y_test_labels = np.around(y_test_pred)\n",
    "y_test_labels_pred = np.argmax(y_test_labels, axis=1)\n",
    "y_test_labels_pred_new = [x-1 for x in y_test_labels_pred]\n",
    "\n",
    "df_test_pred = pd.DataFrame(data=y_test_labels_pred_new, columns=['sentiment'])\n",
    "df_test_pred.to_csv('sentiment_prediction.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
